{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c4ad0-8191-45e6-af19-50667f06378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.core.labels as fol\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from PIL import Image \n",
    "from fiftyone import ViewField as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c64ae-1550-47ad-9124-07eb29250279",
   "metadata": {},
   "source": [
    "## CREATING ENV VARIABLE FOR FIFTYONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459866f-3873-4831-b033-11d871fdfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['FIFTYONE_DIR'] = 'your_desired_fiftyone_directory'\n",
    "os.environ['IMAGES_DIR'] = 'your_desired_images_directory'\n",
    "os.environ['ANNOTATIONS_DIR'] = 'your_desired_annotations_directory'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e95f0-ed9b-4180-b606-6b9c3d4e6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiftyone_dir = os.getenv('FIFTYONE_DIR', 'default_fiftyone_directory')\n",
    "images_dir = os.getenv('IMAGES_DIR', 'default_images_directory')\n",
    "annotations_dir = os.getenv('ANNOTATIONS_DIR', 'default_annotations_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ade893-7b21-41ff-b95f-1357155ef800",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e4d63-2c71-49de-ad7c-3197e6e99c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flag = False\n",
    "val_flag = False\n",
    "test_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97855a-b87a-491d-855c-9c5619124bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(bbox, image_width, image_height):\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_center = (x_min + width / 2) / image_width\n",
    "    y_center = (y_min + height / 2) / image_height\n",
    "    width /= image_width\n",
    "    height /= image_height\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "def verify_file_exists(filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(f\"Warning: File not found - {filepath}\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13c06d-19ea-4de3-83c3-666ec56f7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['validation', 'train', 'test']:\n",
    "    target_image_dir = f'./datasets/coco-human-dataset/images/{split}/images/'\n",
    "    target_label_dir = f'./datasets/coco-human-dataset/images/{split}/labels/'\n",
    "    target_ann_dir = f'./datasets/coco-human-dataset/images/{split}/{split}.json'\n",
    "    \n",
    "    os.makedirs(target_image_dir, exist_ok=True)\n",
    "    os.makedirs(target_label_dir, exist_ok=True)\n",
    "    \n",
    "    print('Processing ' + split)\n",
    "\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"coco-2017\",\n",
    "        split=split,\n",
    "        label_types=[\"detections\", \"segmentations\"],\n",
    "        include_id=True,\n",
    "        seed=42,\n",
    "        dataset_name=f\"{split}-custom\"\n",
    "    )    \n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 1, \"name\": \"person\"},\n",
    "        ],\n",
    "    }\n",
    "    annotation_id = 1\n",
    "    sample_count = 0\n",
    "\n",
    "    if split == 'train':\n",
    "        filtered_view = dataset.filter_labels(\"detections\", F(\"label\") == \"person\")\n",
    "        print(f\"Number of samples with 'person' labels: {len(filtered_view)}\")\n",
    "        \n",
    "        for sample in filtered_view:\n",
    "            # if sample_count >= 5000:\n",
    "            #     break\n",
    "            # sample_count += 1\n",
    "\n",
    "            image_path = sample.filepath\n",
    "            detections = sample.detections.detections\n",
    "            \n",
    "            if detections is None or len(detections) == 0:\n",
    "                print(f\"Skipping {image_path} - no detections\")\n",
    "                continue\n",
    "            \n",
    "            image = Image.open(image_path)\n",
    "            image_width, image_height = image.size\n",
    "\n",
    "            # Copy image to output directory\n",
    "            shutil.copy(image_path, target_image_dir)\n",
    "            \n",
    "            # Extract numeric ID from the file name (e.g., 000000000036.jpg -> 36)\n",
    "            file_name = os.path.basename(image_path)\n",
    "            numeric_id = int(file_name.split('.')[0])\n",
    "            \n",
    "            # Add image info to COCO annotations\n",
    "            coco_annotations[\"images\"].append({\n",
    "                \"id\": numeric_id,\n",
    "                \"file_name\": file_name,\n",
    "                \"width\": image_width,\n",
    "                \"height\": image_height,\n",
    "            })\n",
    "            \n",
    "            # Add detections to COCO annotations\n",
    "            yolo_labels = []\n",
    "            for detection in detections:\n",
    "                if detection.label == \"person\":\n",
    "                    bbox = detection.bounding_box  # [x_min, y_min, width, height]\n",
    "                    x_center, y_center, width, height = normalize_bbox(\n",
    "                        (bbox[0] * image_width, bbox[1] * image_height, bbox[2] * image_width, bbox[3] * image_height),\n",
    "                        image_width, image_height\n",
    "                    )\n",
    "                    yolo_label = f\"0 {x_center} {y_center} {width} {height}\"\n",
    "                    yolo_labels.append(yolo_label)\n",
    "                    \n",
    "                    x_min = bbox[0] * image_width\n",
    "                    y_min = bbox[1] * image_height\n",
    "                    coco_annotations[\"annotations\"].append({\n",
    "                        \"id\": annotation_id,\n",
    "                        \"image_id\": numeric_id,\n",
    "                        \"category_id\": 1,\n",
    "                        \"bbox\": [x_min, y_min, bbox[2] * image_width, bbox[3] * image_height],\n",
    "                        \"area\": bbox[2] * image_width * bbox[3] * image_height,\n",
    "                        \"iscrowd\": 0,\n",
    "                    })\n",
    "                    annotation_id += 1\n",
    "            \n",
    "            # Write YOLO labels to file\n",
    "            label_path = os.path.join(target_label_dir, file_name.replace('.jpg', '.txt'))\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(yolo_labels))\n",
    "        \n",
    "        # Save COCO annotations to file\n",
    "        with open(target_ann_dir, 'w') as f:\n",
    "            json.dump(coco_annotations, f)\n",
    "    \n",
    "    if split == 'test':\n",
    "        for sample in dataset:\n",
    "            # if sample_count >= 5000:\n",
    "            #     break\n",
    "            # sample_count += 1\n",
    "\n",
    "            image_path = sample.filepath\n",
    "            shutil.copy(image_path, target_image_dir)\n",
    "    \n",
    "    if split == 'validation':\n",
    "        images_dir = os.path.join(fiftyone_dir, 'validation', 'data')\n",
    "        annotations_path = os.path.join(fiftyone_dir, 'raw', 'instances_val2017.json')\n",
    "        with open(annotations_path, \"r\") as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in coco_data[\"images\"]}\n",
    "        person_annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"category_id\"] == 1]\n",
    "        person_image_ids = set(ann[\"image_id\"] for ann in person_annotations)\n",
    "\n",
    "        for image_id in person_image_ids:\n",
    "            # if sample_count >= 5000:\n",
    "            #     break\n",
    "            # sample_count += 1\n",
    "\n",
    "            image_filename = image_id_to_filename[image_id]\n",
    "            image_path = os.path.join(images_dir, image_filename)\n",
    "            \n",
    "            if not verify_file_exists(image_path):\n",
    "                continue\n",
    "            \n",
    "            shutil.copy(image_path, target_image_dir)\n",
    "            \n",
    "            # Get annotations for this image\n",
    "            annotations = [ann for ann in person_annotations if ann[\"image_id\"] == image_id]\n",
    "            \n",
    "            # Prepare YOLO format labels\n",
    "            yolo_labels = []\n",
    "            for ann in annotations:\n",
    "                bbox = ann[\"bbox\"]  # [x_min, y_min, width, height]\n",
    "                x_center, y_center, width, height = normalize_bbox(\n",
    "                    bbox, coco_data['images'][0]['width'], coco_data['images'][0]['height']\n",
    "                )\n",
    "                yolo_label = f\"0 {x_center} {y_center} {width} {height}\"\n",
    "                yolo_labels.append(yolo_label)\n",
    "            \n",
    "            # Write YOLO labels to file\n",
    "            label_path = os.path.join(target_label_dir, image_filename.replace('.jpg', '.txt'))\n",
    "            with open(label_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(yolo_labels))\n",
    "            \n",
    "            # Add image info to COCO annotations\n",
    "            coco_annotations[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": image_filename,\n",
    "                \"width\": coco_data['images'][0]['width'],\n",
    "                \"height\": coco_data['images'][0]['height'],\n",
    "            })\n",
    "            \n",
    "            # Add detections to COCO annotations\n",
    "            for ann in annotations:\n",
    "                bbox = ann[\"bbox\"]\n",
    "                x_min = bbox[0]\n",
    "                y_min = bbox[1]\n",
    "                width = bbox[2]\n",
    "                height = bbox[3]\n",
    "                coco_annotations[\"annotations\"].append({\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": 1,\n",
    "                    \"bbox\": [x_min, y_min, width, height],\n",
    "                    \"area\": width * height,\n",
    "                    \"iscrowd\": 0,\n",
    "                })\n",
    "                annotation_id += 1\n",
    "        \n",
    "        # Save COCO annotations for Faster R-CNN\n",
    "        with open(target_ann_dir, 'w') as f:\n",
    "            json.dump(coco_annotations, f)\n",
    "\n",
    "        print(\"Filtered validation images and labels have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97176e1e-9e97-4248-8458-1d7a4d286ae8",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabc526-86c7-476d-8653-7f6bb3eb9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8e9d8-a51e-4087-b4ac-6821804a5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your CUDA and PyTorch installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54ce12-a2e8-4782-b9d6-12f9f200a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yolo = YOLO(\"yolov9c.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb23b0-f998-4c46-ae32-c7e84785d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_yolo.train(data=\"C:/Users/azerr/AIFI/HumanDetectionAI/dataset.yaml\", epochs=1, imgsz=640, device='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfa7e6-e573-43d1-9b36-702f5c7b2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yolo.save(\"trained_yolov9c_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af17fa5-8630-4019-95ca-ec8f1440b3e8",
   "metadata": {},
   "source": [
    "## Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ed6dd-9324-4ceb-ac6c-2165d2038d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import FasterRCNN , fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c42c3-f28b-4d28-8985-7249eedab3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "detection_module_path = ''\n",
    "sys.path.append(detection_module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7ea1b-9e56-4b6a-9dee-5bd60d72b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import utils, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e688b-867f-4d7f-bd95-30ea2fbba8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "picture_range = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37438db9-efa9-42e2-994c-387081e0323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# COCO dataset class names\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n",
    "    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
    "    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "    'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock',\n",
    "    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Function to get predictions for a single image\n",
    "def get_person_detections(image, threshold=0.5):\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    \n",
    "    pred_classes = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in predictions[0]['labels'].cpu().numpy()]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in predictions[0]['boxes'].cpu().numpy()]\n",
    "    pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "    person_boxes = [box for idx, box in enumerate(pred_boxes) if pred_classes[idx] == 'person' and pred_scores[idx] > threshold]\n",
    "    return person_boxes\n",
    "\n",
    "# Function to plot the bounding boxes\n",
    "def plot_detections(image, boxes):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box in boxes:\n",
    "        rect = plt.Rectangle(box[0], box[1][0] - box[0][0], box[1][1] - box[0][1], fill=False, color='red')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# Load and display an image\n",
    "image_path = r\"\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Get person detections\n",
    "person_boxes = get_person_detections(image, threshold=0.5)\n",
    "\n",
    "# Plot the detections\n",
    "plot_detections(image, person_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c31ac-5a00-4fb4-9376-f3018f75b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Function to get predictions for a single image\n",
    "def get_predictions(image):\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    return predictions\n",
    "\n",
    "# Function to convert predictions to COCO format\n",
    "def convert_to_coco_format(predictions, image_id):\n",
    "    coco_predictions = []\n",
    "    for i, box in enumerate(predictions[0]['boxes']):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width, height = xmax - xmin, ymax - ymin\n",
    "        score = predictions[0]['scores'][i].item()\n",
    "        category_id = predictions[0]['labels'][i].item()\n",
    "        coco_predictions.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category_id,\n",
    "            \"bbox\": [xmin.item(), ymin.item(), width.item(), height.item()],\n",
    "            \"score\": score\n",
    "        })\n",
    "    return coco_predictions\n",
    "\n",
    "# Load COCO annotations\n",
    "coco_gt = COCO(r\"C:\\Users\\azerr\\AIFI\\HumanDetectionAI\\datasets\\coco-human-dataset\\images\\val\\validation.json\")\n",
    "\n",
    "# Get image ids from COCO\n",
    "image_ids = coco_gt.getImgIds()\n",
    "\n",
    "# Run inference and collect predictions\n",
    "all_predictions = []\n",
    "start_time = time.time()\n",
    "for image_id in image_ids:\n",
    "    image_info = coco_gt.loadImgs(image_id)[0]\n",
    "    image_path = r\"C:\\Users\\azerr\\AIFI\\HumanDetectionAI\\datasets\\coco-human-dataset\\images\\val\\images\\\\\" + \"\\\\\" + image_info['file_name']\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    predictions = get_predictions(image)\n",
    "    coco_predictions = convert_to_coco_format(predictions, image_id)\n",
    "    all_predictions.extend(coco_predictions)\n",
    "\n",
    "# Save predictions to a JSON file\n",
    "with open('predictions.json', 'w') as f:\n",
    "    json.dump(all_predictions, f)\n",
    "\n",
    "# Load predictions\n",
    "coco_dt = coco_gt.loadRes('predictions.json')\n",
    "\n",
    "# Initialize COCOeval object\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "\n",
    "# Evaluate\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "print(f'Inference time: {time.time() - start_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becd437-1280-4605-8d61-00fe45f924ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = T.Compose([\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CocoDetection(\n",
    "    root='./datasets/coco-human-dataset/images/train/images',\n",
    "    annFile='./datasets/coco-human-dataset/images/train/train.json',\n",
    "    transform=data_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b888df-5016-4a43-b1ef-dfe91c7f907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c597a-d38f-4117-821f-6213d1079474",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CocoDetection(\n",
    "    root='./datasets/coco-human-dataset/images/val/images',\n",
    "    annFile='./datasets/coco-human-dataset/images/val/validation.json',\n",
    "    transform=data_transforms\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfa835-1305-455c-a7be-3611beeeca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate Scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Function to filter invalid bounding boxes\n",
    "def filter_invalid_boxes(target, device):\n",
    "    valid_boxes = []\n",
    "    valid_labels = []\n",
    "    valid_area = []\n",
    "    valid_iscrowd = []\n",
    "    \n",
    "    for i, box in enumerate(target['boxes']):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        if xmax > xmin and ymax > ymin:  # Ensure positive width and height\n",
    "            valid_boxes.append(box)\n",
    "            valid_labels.append(target['labels'][i])\n",
    "            valid_area.append(target['area'][i])\n",
    "            valid_iscrowd.append(target['iscrowd'][i])\n",
    "    \n",
    "    target['boxes'] = torch.stack(valid_boxes).to(device)\n",
    "    target['labels'] = torch.as_tensor(valid_labels, dtype=torch.int64).to(device)\n",
    "    target['area'] = torch.as_tensor(valid_area, dtype=torch.float32).to(device)\n",
    "    target['iscrowd'] = torch.as_tensor(valid_iscrowd, dtype=torch.int64).to(device)\n",
    "    \n",
    "    return target\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc639dea-6b93-4adc-a076-7f2bdb7b0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def cust_evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    coco = COCO('./datasets/coco-human-dataset/images/val/validation.json')\n",
    "    coco_evaluator = COCOeval(coco, coco, 'bbox')\n",
    "\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    results = []\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "\n",
    "        for target_list, output in zip(targets, outputs):\n",
    "            for target in target_list:\n",
    "                image_id = target[\"image_id\"]\n",
    "                for i in range(len(output[\"boxes\"])):\n",
    "                    box = output[\"boxes\"][i].tolist()\n",
    "                    score = output[\"scores\"][i].item()\n",
    "                    category_id = output[\"labels\"][i].item()\n",
    "                    result = {\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"bbox\": [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "                    results.append(result)\n",
    "\n",
    "    # Load results into COCOeval\n",
    "    coco_results = coco.loadRes(results)\n",
    "    coco_evaluator = COCOeval(coco, coco_results, 'bbox')\n",
    "\n",
    "    coco_evaluator.evaluate()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6464473-713f-4cf0-8ef0-6d0efbbbac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print('Epoch Starting')\n",
    "    with torch.inference_mode():\n",
    "        model.train()  # Set to training mode\n",
    "        for images, targets in train_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            \n",
    "            # Modify targets to fit the expected format\n",
    "            formatted_targets = []\n",
    "            for t in targets:\n",
    "                boxes = []\n",
    "                labels = []\n",
    "                area = []\n",
    "                iscrowd = []\n",
    "    \n",
    "                for obj in t:\n",
    "                    xmin, ymin, width, height = obj['bbox']\n",
    "                    boxes.append([xmin, ymin, xmin + width, ymin + height])\n",
    "                    labels.append(obj['category_id'])\n",
    "                    area.append(obj['area'])\n",
    "                    iscrowd.append(obj['iscrowd'])\n",
    "    \n",
    "                boxes = torch.as_tensor(boxes, dtype=torch.float32) # reccomended float16\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64) # reccomended float32\n",
    "                area = torch.as_tensor(area, dtype=torch.float32)\n",
    "                iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "                image_id = torch.tensor([t[0]['image_id']], dtype=torch.int64)\n",
    "    \n",
    "                formatted_target = {\n",
    "                    'boxes': boxes,\n",
    "                    'labels': labels,\n",
    "                    'area': area,\n",
    "                    'iscrowd': iscrowd,\n",
    "                    'image_id': image_id\n",
    "                }\n",
    "                \n",
    "                # Filter out invalid boxes\n",
    "                formatted_target = filter_invalid_boxes(formatted_target, device)\n",
    "                formatted_targets.append(formatted_target)\n",
    "    \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, formatted_targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "    \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        lr_scheduler.step()\n",
    "    \n",
    "        # Evaluate model after each epoch\n",
    "        cust_evaluate(model, val_loader, device=device)\n",
    "        print('Epoch Completed')\n",
    "        print('==========================')\n",
    "print(\"Training complete!\")\n",
    "torch.save(model.state_dict(), 'trained_yolov9c_weights.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d85e4-e7f9-484f-ac95-d7f141378cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# COCO dataset class names\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n",
    "    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
    "    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "    'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock',\n",
    "    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Function to get predictions for a single image\n",
    "def get_person_detections(image, threshold=0.5):\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    \n",
    "    pred_classes = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in predictions[0]['labels'].cpu().numpy()]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in predictions[0]['boxes'].cpu().numpy()]\n",
    "    pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "    person_boxes = [box for idx, box in enumerate(pred_boxes) if pred_classes[idx] == 'person' and pred_scores[idx] > threshold]\n",
    "    return person_boxes\n",
    "\n",
    "# Function to plot the bounding boxes\n",
    "def plot_detections(image, boxes):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box in boxes:\n",
    "        rect = plt.Rectangle(box[0], box[1][0] - box[0][0], box[1][1] - box[0][1], fill=False, color='red')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# Load and display an image\n",
    "image_path = r\"C:\\Users\\azerr\\Downloads\\6237879203_f785d06a9a_b.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Get person detections\n",
    "person_boxes = get_person_detections(image, threshold=0.5)\n",
    "\n",
    "# Plot the detections\n",
    "plot_detections(image, person_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e38f48-6f91-419d-85b9-dcd00decf3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
